{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation functions play a crucial role in neural networks, determining the output of a neuron and thereby influencing the entire model’s learning process. Here's a detailed explanation of activation functions in machine learning:\n",
    "\n",
    "### 1. **What is an Activation Function?**\n",
    "An activation function defines how the weighted sum of the input signals to a neuron is transformed into an output signal in a neural network. Essentially, it decides whether a neuron should be activated or not, introducing non-linearity into the model, which allows neural networks to learn and model complex data.\n",
    "\n",
    "### 2. **Types of Activation Functions**\n",
    "There are several types of activation functions, each with its own characteristics and use cases:\n",
    "\n",
    "#### 2.1. **Linear Activation Function**\n",
    "- **Function**: \\( f(x) = x \\)\n",
    "- **Properties**: Outputs the input directly without any transformation.\n",
    "- **Pros**: Simplicity and easy computation.\n",
    "- **Cons**: Cannot model non-linear data, making it less effective for complex tasks.\n",
    "- **Use Case**: Typically used in the output layer for regression tasks.\n",
    "\n",
    "#### 2.2. **Step Function**\n",
    "- **Function**: \\( f(x) = 1 \\) if \\( x > 0 \\), otherwise \\( f(x) = 0 \\)\n",
    "- **Properties**: Outputs either 0 or 1 based on the input value.\n",
    "- **Pros**: Simple to understand and implement.\n",
    "- **Cons**: Not differentiable, which is problematic for gradient-based optimization.\n",
    "- **Use Case**: Early neural network models, now largely obsolete.\n",
    "\n",
    "#### 2.3. **Sigmoid Activation Function**\n",
    "- **Function**: \\( f(x) = \\frac{1}{1 + e^{-x}} \\)\n",
    "- **Properties**: Squashes input values between 0 and 1, creating a smooth curve.\n",
    "- **Pros**: Differentiable and outputs probabilities, making it suitable for binary classification.\n",
    "- **Cons**: Prone to the vanishing gradient problem, where gradients become very small, slowing down learning.\n",
    "- **Use Case**: Often used in the output layer of binary classification problems.\n",
    "\n",
    "#### 2.4. **Tanh (Hyperbolic Tangent) Activation Function**\n",
    "- **Function**: \\( f(x) = \\tanh(x) = \\frac{2}{1 + e^{-2x}} - 1 \\)\n",
    "- **Properties**: Similar to the sigmoid function but squashes input values between -1 and 1.\n",
    "- **Pros**: Centered around zero, which helps in convergence during training.\n",
    "- **Cons**: Also suffers from the vanishing gradient problem.\n",
    "- **Use Case**: Common in hidden layers of neural networks.\n",
    "\n",
    "#### 2.5. **ReLU (Rectified Linear Unit) Activation Function**\n",
    "- **Function**: \\( f(x) = \\max(0, x) \\)\n",
    "- **Properties**: Outputs the input directly if positive; otherwise, it outputs zero.\n",
    "- **Pros**: Computationally efficient and helps mitigate the vanishing gradient problem.\n",
    "- **Cons**: Can cause \"dead neurons\" where neurons stop learning if they get stuck in the negative input space.\n",
    "- **Use Case**: Widely used in hidden layers of deep neural networks.\n",
    "\n",
    "#### 2.6. **Leaky ReLU Activation Function**\n",
    "- **Function**: \\( f(x) = x \\) if \\( x > 0 \\), otherwise \\( f(x) = \\alpha x \\) (where \\( \\alpha \\) is a small positive constant, e.g., 0.01)\n",
    "- **Properties**: Similar to ReLU but allows a small, non-zero gradient when the input is negative.\n",
    "- **Pros**: Reduces the risk of dead neurons.\n",
    "- **Cons**: The choice of \\( \\alpha \\) can be arbitrary.\n",
    "- **Use Case**: An alternative to ReLU, especially when the model experiences dead neurons.\n",
    "\n",
    "#### 2.7. **ELU (Exponential Linear Unit) Activation Function**\n",
    "- **Function**: \n",
    "  \\[\n",
    "  f(x) = \n",
    "  \\begin{cases} \n",
    "  x & \\text{if } x > 0 \\\\\n",
    "  \\alpha (e^x - 1) & \\text{if } x \\leq 0 \n",
    "  \\end{cases}\n",
    "  \\]\n",
    "- **Properties**: Similar to ReLU but smooths the negative part of the input.\n",
    "- **Pros**: Reduces the vanishing gradient problem and allows negative values for negative inputs.\n",
    "- **Cons**: Computationally more expensive than ReLU.\n",
    "- **Use Case**: Preferred in deep networks where negative inputs are prevalent.\n",
    "\n",
    "#### 2.8. **Softmax Activation Function**\n",
    "- **Function**: \n",
    "  \\[\n",
    "  f(x_i) = \\frac{e^{x_i}}{\\sum_{j=1}^{n} e^{x_j}}\n",
    "  \\]\n",
    "- **Properties**: Converts a vector of values into a probability distribution.\n",
    "- **Pros**: Outputs probabilities that sum to 1, suitable for multi-class classification.\n",
    "- **Cons**: Can be sensitive to outliers due to the exponential function.\n",
    "- **Use Case**: Commonly used in the output layer for multi-class classification problems.\n",
    "\n",
    "### 3. **Choosing the Right Activation Function**\n",
    "The choice of activation function depends on the problem type and the specific layer within the neural network:\n",
    "\n",
    "- **Input Layer**: Typically, no activation function is used (linear).\n",
    "- **Hidden Layers**: \n",
    "  - ReLU and its variants (Leaky ReLU, ELU) are popular choices due to their ability to mitigate the vanishing gradient problem.\n",
    "  - Tanh or Sigmoid can be used when the data is bounded and you want more smooth transitions.\n",
    "- **Output Layer**:\n",
    "  - **Binary Classification**: Sigmoid is typically used as it provides a probability-like output.\n",
    "  - **Multi-Class Classification**: Softmax is used to output a probability distribution.\n",
    "  - **Regression**: Linear activation is used to output continuous values.\n",
    "\n",
    "### 4. **Why Non-Linearity is Important**\n",
    "Without non-linearity, a neural network with multiple layers would behave like a single-layer perceptron, no matter how many layers it has. Non-linear activation functions allow the network to model complex relationships in the data, which is essential for tasks like image recognition, natural language processing, and more.\n",
    "\n",
    "### 5. **Challenges with Activation Functions**\n",
    "- **Vanishing Gradient Problem**: Sigmoid and Tanh functions can cause gradients to become very small, slowing down or even stopping training.\n",
    "- **Exploding Gradient Problem**: Some activation functions can lead to very large gradients, causing unstable models.\n",
    "- **Dead Neurons**: ReLU can cause some neurons to become inactive, meaning they stop learning if their input is always negative.\n",
    "\n",
    "### 6. **Recent Developments**\n",
    "Recent research has led to the development of advanced activation functions like **Swish** and **Mish**:\n",
    "- **Swish**: \\( f(x) = x \\cdot \\text{sigmoid}(x) \\), combines linearity and non-linearity, showing better performance in deep networks.\n",
    "- **Mish**: \\( f(x) = x \\cdot \\tanh(\\ln(1 + e^x)) \\), is smoother and performs better in certain tasks than ReLU and Swish.\n",
    "\n",
    "### 7. **Summary**\n",
    "Activation functions are a fundamental component of neural networks, enabling them to learn complex patterns and make accurate predictions. The choice of activation function significantly impacts the performance and training of the model, and understanding the strengths and weaknesses of each function is crucial for building effective neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation Functions in Machine Learning\n",
    "Activation functions are a crucial component of neural networks, enabling them to learn complex patterns and relationships in data. They introduce non-linearity into the model, allowing it to approximate a wide range of functions. Here’s a detailed overview of the most common activation functions used in machine learning.\n",
    "\n",
    "1. Sigmoid Function\n",
    "Formula: \n",
    "f\n",
    "(\n",
    "x\n",
    ")\n",
    "=\n",
    "1\n",
    "1\n",
    "+\n",
    "e\n",
    "−\n",
    "x\n",
    "f(x)= \n",
    "1+e \n",
    "−x\n",
    " \n",
    "1\n",
    "​\n",
    " \n",
    "Range: (0, 1)\n",
    "Characteristics:\n",
    "Smooth gradient, which helps in optimization.\n",
    "Output values are between 0 and 1, making it suitable for binary classification.\n",
    "Drawbacks: Prone to the vanishing gradient problem, where gradients become very small, slowing down learning.\n",
    "Sigmoid Function\n",
    "\n",
    "2. Tanh Function\n",
    "Formula: \n",
    "f\n",
    "(\n",
    "x\n",
    ")\n",
    "=\n",
    "tanh\n",
    "⁡\n",
    "(\n",
    "x\n",
    ")\n",
    "=\n",
    "e\n",
    "x\n",
    "−\n",
    "e\n",
    "−\n",
    "x\n",
    "e\n",
    "x\n",
    "+\n",
    "e\n",
    "−\n",
    "x\n",
    "f(x)=tanh(x)= \n",
    "e \n",
    "x\n",
    " +e \n",
    "−x\n",
    " \n",
    "e \n",
    "x\n",
    " −e \n",
    "−x\n",
    " \n",
    "​\n",
    " \n",
    "Range: (-1, 1)\n",
    "Characteristics:\n",
    "Zero-centered, which helps in faster convergence.\n",
    "Like the sigmoid, it can also suffer from the vanishing gradient problem.\n",
    "Tanh Function\n",
    "\n",
    "3. ReLU (Rectified Linear Unit)\n",
    "Formula: \n",
    "f\n",
    "(\n",
    "x\n",
    ")\n",
    "=\n",
    "max\n",
    "⁡\n",
    "(\n",
    "0\n",
    ",\n",
    "x\n",
    ")\n",
    "f(x)=max(0,x)\n",
    "Range: [0, ∞)\n",
    "Characteristics:\n",
    "Computationally efficient and helps mitigate the vanishing gradient problem.\n",
    "Can lead to dead neurons (neurons that stop learning) if inputs are negative.\n",
    "ReLU Function\n",
    "\n",
    "4. Leaky ReLU\n",
    "Formula: \n",
    "f\n",
    "(\n",
    "x\n",
    ")\n",
    "=\n",
    "max\n",
    "⁡\n",
    "(\n",
    "0.01\n",
    "x\n",
    ",\n",
    "x\n",
    ")\n",
    "f(x)=max(0.01x,x)\n",
    "Range: (-∞, ∞)\n",
    "Characteristics:\n",
    "A variant of ReLU that allows a small, non-zero gradient when the input is negative.\n",
    "Helps prevent dead neurons.\n",
    "Leaky ReLU Function\n",
    "\n",
    "5. Softmax Function\n",
    "Formula: \n",
    "f\n",
    "(\n",
    "x\n",
    "i\n",
    ")\n",
    "=\n",
    "e\n",
    "x\n",
    "i\n",
    "∑\n",
    "j\n",
    "e\n",
    "x\n",
    "j\n",
    "f(x \n",
    "i\n",
    "​\n",
    " )= \n",
    "∑ \n",
    "j\n",
    "​\n",
    " e \n",
    "x \n",
    "j\n",
    "​\n",
    " \n",
    " \n",
    "e \n",
    "x \n",
    "i\n",
    "​\n",
    " \n",
    " \n",
    "​\n",
    " \n",
    "Range: (0, 1) for each class, summing to 1.\n",
    "Characteristics:\n",
    "Used in multi-class classification problems.\n",
    "Converts logits (raw prediction scores) into probabilities.\n",
    "Softmax Function\n",
    "\n",
    "Summary of Activation Functions\n",
    "Function\tFormula\tRange\tUse Case\n",
    "Sigmoid\t\n",
    "1\n",
    "1\n",
    "+\n",
    "e\n",
    "−\n",
    "x\n",
    "1+e \n",
    "−x\n",
    " \n",
    "1\n",
    "​\n",
    " \t(0, 1)\tBinary classification\n",
    "Tanh\t\n",
    "tanh\n",
    "⁡\n",
    "(\n",
    "x\n",
    ")\n",
    "tanh(x)\t(-1, 1)\tHidden layers\n",
    "ReLU\t\n",
    "max\n",
    "⁡\n",
    "(\n",
    "0\n",
    ",\n",
    "x\n",
    ")\n",
    "max(0,x)\t[0, ∞)\tHidden layers\n",
    "Leaky ReLU\t\n",
    "max\n",
    "⁡\n",
    "(\n",
    "0.01\n",
    "x\n",
    ",\n",
    "x\n",
    ")\n",
    "max(0.01x,x)\t(-∞, ∞)\tHidden layers\n",
    "Softmax\t\n",
    "e\n",
    "x\n",
    "i\n",
    "∑\n",
    "j\n",
    "e\n",
    "x\n",
    "j\n",
    "∑ \n",
    "j\n",
    "​\n",
    " e \n",
    "x \n",
    "j\n",
    "​\n",
    " \n",
    " \n",
    "e \n",
    "x \n",
    "i\n",
    "​\n",
    " \n",
    " \n",
    "​\n",
    " \t(0, 1)\tMulti-class classification\n",
    "Conclusion\n",
    "Activation functions play a vital role in the performance of neural networks. Choosing the right activation function can significantly impact the model's ability to learn and generalize from data. Understanding their properties and appropriate use cases is essential for building effective machine learning models.\n",
    "\n",
    "For further reading, you can explore these resources:\n",
    "\n",
    "Introduction to Activation Functions in Neural Networks\n",
    "Activation Functions | Machine Learning Geek\n",
    "Activation Functions in Neural Networks [12 Types & Use Cases]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples\n",
    "\n",
    "Below are examples of how to implement various activation functions in Python using libraries like NumPy and TensorFlow. Each code snippet includes a brief explanation of the function.\n",
    "\n",
    "### 1. **Sigmoid Function**\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Example usage\n",
    "x = np.array([-2, -1, 0, 1, 2])\n",
    "sigmoid_output = sigmoid(x)\n",
    "print(\"Sigmoid Output:\", sigmoid_output)\n",
    "```\n",
    "\n",
    "### 2. **Tanh Function**\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "# Example usage\n",
    "x = np.array([-2, -1, 0, 1, 2])\n",
    "tanh_output = tanh(x)\n",
    "print(\"Tanh Output:\", tanh_output)\n",
    "```\n",
    "\n",
    "### 3. **ReLU (Rectified Linear Unit)**\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# Example usage\n",
    "x = np.array([-2, -1, 0, 1, 2])\n",
    "relu_output = relu(x)\n",
    "print(\"ReLU Output:\", relu_output)\n",
    "```\n",
    "\n",
    "### 4. **Leaky ReLU**\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def leaky_relu(x, alpha=0.01):\n",
    "    return np.where(x > 0, x, alpha * x)\n",
    "\n",
    "# Example usage\n",
    "x = np.array([-2, -1, 0, 1, 2])\n",
    "leaky_relu_output = leaky_relu(x)\n",
    "print(\"Leaky ReLU Output:\", leaky_relu_output)\n",
    "```\n",
    "\n",
    "### 5. **Softmax Function**\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x))  # for numerical stability\n",
    "    return exp_x / exp_x.sum(axis=0)\n",
    "\n",
    "# Example usage\n",
    "x = np.array([2.0, 1.0, 0.1])\n",
    "softmax_output = softmax(x)\n",
    "print(\"Softmax Output:\", softmax_output)\n",
    "```\n",
    "\n",
    "### TensorFlow Implementations\n",
    "If you are using TensorFlow, you can utilize built-in functions for activation functions:\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "# Example input\n",
    "x = tf.constant([-2.0, -1.0, 0.0, 1.0, 2.0])\n",
    "\n",
    "# Sigmoid\n",
    "sigmoid_output_tf = tf.sigmoid(x)\n",
    "print(\"TensorFlow Sigmoid Output:\", sigmoid_output_tf.numpy())\n",
    "\n",
    "# Tanh\n",
    "tanh_output_tf = tf.tanh(x)\n",
    "print(\"TensorFlow Tanh Output:\", tanh_output_tf.numpy())\n",
    "\n",
    "# ReLU\n",
    "relu_output_tf = tf.nn.relu(x)\n",
    "print(\"TensorFlow ReLU Output:\", relu_output_tf.numpy())\n",
    "\n",
    "# Leaky ReLU\n",
    "leaky_relu_output_tf = tf.nn.leaky_relu(x, alpha=0.01)\n",
    "print(\"TensorFlow Leaky ReLU Output:\", leaky_relu_output_tf.numpy())\n",
    "\n",
    "# Softmax\n",
    "softmax_output_tf = tf.nn.softmax(x)\n",
    "print(\"TensorFlow Softmax Output:\", softmax_output_tf.numpy())\n",
    "```\n",
    "\n",
    "### Visualizing Activation Functions\n",
    "You can visualize these activation functions using Matplotlib:\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.linspace(-10, 10, 100)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.plot(x, sigmoid(x), label='Sigmoid')\n",
    "plt.title('Sigmoid Function')\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.plot(x, tanh(x), label='Tanh', color='orange')\n",
    "plt.title('Tanh Function')\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.plot(x, relu(x), label='ReLU', color='green')\n",
    "plt.title('ReLU Function')\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(2, 3, 4)\n",
    "plt.plot(x, leaky_relu(x), label='Leaky ReLU', color='red')\n",
    "plt.title('Leaky ReLU Function')\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.plot(x, softmax(x - np.max(x)), label='Softmax', color='purple')  # Shift for numerical stability\n",
    "plt.title('Softmax Function')\n",
    "plt.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Conclusion\n",
    "These code snippets provide a practical implementation of various activation functions in both NumPy and TensorFlow. You can use these functions in your neural network models to enhance their learning capabilities. If you have any further questions or need additional examples, feel free to ask!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
